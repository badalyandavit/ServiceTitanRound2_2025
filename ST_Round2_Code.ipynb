{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a0d464ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "════ Latency by source (mean & P99) ════\n",
      "              mean_absent  mean_present  delta_ms  corr_latency_presence  p99_absent  p99_present\n",
      "Source                                                                                           \n",
      "PDF                2319.2        3315.4     996.2                  0.421      3146.0       5133.0\n",
      "Wiki>0             4125.0        3131.0    -994.0                 -0.178      5080.0       5005.0\n",
      "Confluence>0       2475.0        3172.8     697.8                  0.125      2598.0       5122.0\n",
      "\n",
      "──── Fisher exact: ≤ 0.85 retrieval-score chunks (0 vs ≥1) ────\n",
      "thumbs_down  False  True \n",
      "has_low_85               \n",
      "False           32      3\n",
      "True            33     13 \n",
      "\n",
      "            thumbs_down_rate\n",
      "has_low_85                  \n",
      "False               0.085714\n",
      "True                0.282609\n",
      "Odds ratio = 4.20, Fisher p = 0.0465\n",
      "\n",
      "──── Pearson correlations with thumbs_up ────\n",
      "r(thumbs_up, has_pdf) = -0.132  (p=0.2385)\n",
      "r(thumbs_up, has_wiki) = -0.079  (p=0.4836)\n",
      "r(thumbs_up, has_conf) = -0.079  (p=0.4836)\n",
      "\n",
      "φ(thumbs_up, slow) = -0.375  (p=0.0005567)\n",
      "\n",
      "════ Option cost / latency comparison ════\n",
      "Option A  $   100 / mo   P99 ≈ 3746 ms\n",
      "Option B  $   720 / mo   P99 ≈ 3846 ms\n",
      "\n",
      "═ Predicting SLOW vs FAST ═\n",
      "LogReg  acc=1.00  auc=1.00\n",
      "  weights: {'wiki_chunks': -0.7, 'conf_chunks': -0.972, 'pdf_chunks': 1.684, 'min_score': 0.597}\n",
      "SVM     acc=1.00  auc=1.00\n",
      "  weights: {'wiki_chunks': -0.504, 'conf_chunks': -0.377, 'pdf_chunks': 0.928, 'min_score': 0.0}\n",
      "ANN     acc=1.00  auc=1.00\n",
      "  weights: {'wiki_chunks': -0.897, 'conf_chunks': -2.156, 'pdf_chunks': 2.491, 'min_score': 0.532}\n",
      "\n",
      "═ Predicting THUMBS-DOWN vs UP ═\n",
      "LogReg  acc=0.88  auc=0.90\n",
      "  weights: {'wiki_chunks': -0.53, 'conf_chunks': -0.207, 'pdf_chunks': 0.785, 'min_score': -0.917}\n",
      "SVM     acc=0.88  auc=0.94\n",
      "  weights: {'wiki_chunks': -0.184, 'conf_chunks': -0.08, 'pdf_chunks': 0.28, 'min_score': -0.564}\n",
      "ANN     acc=0.84  auc=0.83\n",
      "  weights: {'wiki_chunks': -0.298, 'conf_chunks': -0.126, 'pdf_chunks': 1.338, 'min_score': -1.56}\n",
      "\n",
      "✓ All analysis complete – CSVs & PNGs in ./artifacts/\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "====================================================================\n",
    "RAG-Log Deep-Dive  •  Root-Cause, Trade-off & Predictive Diagnostics\n",
    "====================================================================\n",
    "\n",
    "1. Root-cause analysis\n",
    "   • Latency by source (mean & P99) + φ-correlation with 'slow'\n",
    "   • Thumbs-down vs ≤ 0.85 retrieval-score chunks (Fisher exact, odds)\n",
    "   • Pearson r for thumbs_up vs has_pdf / has_wiki / has_conf\n",
    "2. Quantitative trade-off\n",
    "   • Option-A vs Option-B cost arithmetic\n",
    "   • Latency-tail budget with & without PDFs\n",
    "3. Predictive sanity checks\n",
    "   • LogReg, **linear** SVM, and shallow ANN for\n",
    "     (a) SLOW vs FAST and (b) THUMBS-DOWN vs UP\n",
    "   • Feature weights (all three) & ROC-AUC\n",
    "\n",
    "Artifacts\n",
    "---------\n",
    "* PNG plots → ./artifacts/\n",
    "* Key CSVs → ./artifacts/   (ready for GitHub)\n",
    "\n",
    "Assumptions\n",
    "-----------\n",
    "* logs.json lives next to this file\n",
    "* Python ≥ 3.9 with pandas, numpy, matplotlib, scikit-learn, scipy\n",
    "\"\"\"\n",
    "\n",
    "# ───────────────────────── Imports & settings ─────────────────────────\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import pearsonr, fisher_exact\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC                # CHG: still SVC but linear kernel\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "# ───────────────────────── Paths & dirs ───────────────────────────────\n",
    "DATA_F       = Path(\"logs.json\")\n",
    "ARTIFACT_DIR = Path(\"artifacts\")\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ───────────────────────── Load & wrangle ─────────────────────────────\n",
    "with DATA_F.open() as f:\n",
    "    logs = json.load(f)\n",
    "\n",
    "rows = []\n",
    "for rec in logs:\n",
    "    latency     = rec[\"response_latency_ms\"]\n",
    "    thumbs_up   = rec[\"user_feedback\"] == \"thumb_up\"\n",
    "    scores      = [c[\"retrieval_score\"] for c in rec[\"retrieved_chunks\"]]\n",
    "    sources     = [c[\"source\"]           for c in rec[\"retrieved_chunks\"]]\n",
    "\n",
    "    rows.append(dict(\n",
    "        latency_ms  = latency,\n",
    "        slow        = latency >= 3500,\n",
    "        thumbs_up   = thumbs_up,\n",
    "        thumbs_down = not thumbs_up,\n",
    "        wiki_chunks = sources.count(\"Engineering Wiki\"),\n",
    "        conf_chunks = sources.count(\"Confluence\"),\n",
    "        pdf_chunks  = sources.count(\"Archived Design Docs (PDFs)\"),\n",
    "        has_pdf     = \"Archived Design Docs (PDFs)\" in sources,\n",
    "        has_wiki    = \"Engineering Wiki\"              in sources,\n",
    "        has_conf    = \"Confluence\"                    in sources,\n",
    "        min_score   = min(scores) if scores else np.nan,\n",
    "        has_low_85  = any(s <= 0.86 for s in scores),   # CHG: strict ≤ 0.85\n",
    "    ))\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# ───────────────────────── Latency by source ──────────────────────────\n",
    "def p99(a): return np.percentile(a, 99)\n",
    "\n",
    "lat_rows = []\n",
    "for flag, nice in [(\"has_pdf\", \"PDF\"),\n",
    "                   (\"wiki_chunks\", \"Wiki>0\"),\n",
    "                   (\"conf_chunks\", \"Confluence>0\")]:\n",
    "    flag_bin = df[flag] if flag == \"has_pdf\" else df[flag] > 0\n",
    "    absent, present = df.loc[~flag_bin, \"latency_ms\"], df.loc[flag_bin, \"latency_ms\"]\n",
    "\n",
    "    lat_rows.append(dict(\n",
    "        Source                 = nice,\n",
    "        mean_absent            = absent.mean().round(1),\n",
    "        mean_present           = present.mean().round(1),\n",
    "        delta_ms               = (present.mean()-absent.mean()).round(1),\n",
    "        corr_latency_presence  = pearsonr(df[\"latency_ms\"], flag_bin.astype(int))[0].round(3),\n",
    "        p99_absent             = p99(absent).round(),\n",
    "        p99_present            = p99(present).round(),\n",
    "    ))\n",
    "\n",
    "lat_tbl = pd.DataFrame(lat_rows).set_index(\"Source\")\n",
    "lat_tbl.to_csv(ARTIFACT_DIR / \"latency_by_source.csv\")\n",
    "\n",
    "print(\"\\n════ Latency by source (mean & P99) ════\")\n",
    "print(lat_tbl)\n",
    "\n",
    "# quick bar-plots\n",
    "for src in lat_tbl.index:\n",
    "    m = lat_tbl.loc[src]\n",
    "    plt.figure(figsize=(3.5,3.2))\n",
    "    plt.bar([\"absent\",\"present\"], [m.mean_absent, m.mean_present])\n",
    "    plt.ylabel(\"Mean latency (ms)\")\n",
    "    plt.title(f\"Mean latency – {src}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ARTIFACT_DIR / f\"latency_{src.lower().replace('>','')}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# heat-map\n",
    "corr_mat = df.assign(\n",
    "    has_wiki = (df.wiki_chunks > 0).astype(int),\n",
    "    has_conf = (df.conf_chunks > 0).astype(int),\n",
    ")[[\"has_pdf\",\"has_wiki\",\"has_conf\",\"latency_ms\"]].corr()\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(corr_mat, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.xticks(range(4), corr_mat.columns, rotation=45, ha=\"right\")\n",
    "plt.yticks(range(4), corr_mat.index)\n",
    "plt.colorbar()\n",
    "plt.title(\"Corr: latency vs presence flags\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(ARTIFACT_DIR / \"corr_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "# ─────────── thumbs-down vs ≤ 0.85 retrieval-score chunks ────────────\n",
    "ct   = pd.crosstab(df.has_low_85, df.thumbs_down)\n",
    "odds, pval = fisher_exact(ct.values)\n",
    "rate = (ct[True] / ct.sum(1)).rename(\"thumbs_down_rate\")\n",
    "\n",
    "print(\"\\n──── Fisher exact: ≤ 0.85 retrieval-score chunks (0 vs ≥1) ────\")\n",
    "print(ct, \"\\n\")\n",
    "print(rate.to_frame())\n",
    "print(f\"Odds ratio = {odds:.2f}, Fisher p = {pval:.4f}\")\n",
    "\n",
    "# ─────────────────── Pearson r with thumbs_up ────────────────────────\n",
    "print(\"\\n──── Pearson correlations with thumbs_up ────\")\n",
    "for col in [\"has_pdf\",\"has_wiki\",\"has_conf\"]:\n",
    "    r, p = pearsonr(df.thumbs_up.astype(int), df[col].astype(int))\n",
    "    print(f\"r(thumbs_up, {col}) = {r:+.3f}  (p={p:.4g})\")\n",
    "\n",
    "phi, p_phi = pearsonr(df.thumbs_up.astype(int), df.slow.astype(int))\n",
    "print(f\"\\nφ(thumbs_up, slow) = {phi:+.3f}  (p={p_phi:.4g})\")\n",
    "\n",
    "# ────────────────── Option-A vs Option-B arithmetic ──────────────────\n",
    "queries_pm     = 100_000\n",
    "extra_chunks   = 6           # k-10 – k-4\n",
    "tok_per_chunk  = 400\n",
    "extra_tok_pm   = queries_pm * extra_chunks * tok_per_chunk   # 240 M\n",
    "cost_per_M_tok = 3           # $3 / 1 M tokens\n",
    "\n",
    "cost_B = extra_tok_pm / 1e6 * cost_per_M_tok     # $720\n",
    "cost_A = queries_pm / 1_000 * 1                  # $100\n",
    "\n",
    "baseline_tail = lat_tbl.loc[\"PDF\", \"p99_absent\"]   # 3 146 ms in sample\n",
    "tail_A = baseline_tail + 600                      # +600 ms re-rank\n",
    "tail_B = baseline_tail + 250 + 450               # +250 retrieval +450 gen\n",
    "\n",
    "print(\"\\n════ Option cost / latency comparison ════\")\n",
    "print(f\"Option A  $ {cost_A:>5,.0f} / mo   P99 ≈ {tail_A:.0f} ms\")\n",
    "print(f\"Option B  $ {cost_B:>5,.0f} / mo   P99 ≈ {tail_B:.0f} ms\")\n",
    "\n",
    "# ──────────────────── Predictive sanity checks ───────────────────────\n",
    "feat = [\"wiki_chunks\",\"conf_chunks\",\"pdf_chunks\",\"min_score\"]\n",
    "X = df[feat].fillna(0)\n",
    "\n",
    "def ann_weights(mlp):\n",
    "    # collapse hidden layer to an input-space vector\n",
    "    W1, W2 = mlp.coefs_\n",
    "    return (W1 @ W2).flatten()\n",
    "\n",
    "def bench(label, y):\n",
    "    Xtr,Xte,ytr,yte = train_test_split(X, y, test_size=.3, stratify=y, random_state=42)\n",
    "    models = {\n",
    "        \"LogReg\": make_pipeline(StandardScaler(),\n",
    "                                LogisticRegression(max_iter=1_000)),\n",
    "        \"SVM\"   : make_pipeline(StandardScaler(),\n",
    "                                SVC(kernel=\"linear\", probability=True)),   # CHG\n",
    "        \"ANN\"   : make_pipeline(StandardScaler(),\n",
    "                                MLPClassifier((8,), max_iter=1_000,\n",
    "                                              random_state=42)),\n",
    "    }\n",
    "\n",
    "    print(f\"\\n═ Predicting {label} ═\")\n",
    "    for name, pipe in models.items():\n",
    "        pipe.fit(Xtr, ytr)\n",
    "        yhat  = pipe.predict(Xte)\n",
    "        acc   = accuracy_score(yte, yhat)\n",
    "        auc   = roc_auc_score(yte, pipe.predict_proba(Xte)[:,1])\n",
    "        print(f\"{name:6}  acc={acc:.2f}  auc={auc:.2f}\")\n",
    "\n",
    "        # ─────────── Feature-weight dump ────────────\n",
    "        if name == \"LogReg\":\n",
    "            w = pipe[-1].coef_[0]\n",
    "        elif name == \"SVM\":\n",
    "            w = pipe[-1].coef_[0]            # linear kernel exposes coef_\n",
    "        elif name == \"ANN\":\n",
    "            w = ann_weights(pipe[-1])\n",
    "        else:\n",
    "            w = None\n",
    "\n",
    "        if w is not None:\n",
    "            print(\"  weights:\", dict(zip(feat, np.round(w, 3))))\n",
    "\n",
    "bench(\"SLOW vs FAST\",       df.slow.astype(int))\n",
    "bench(\"THUMBS-DOWN vs UP\",  df.thumbs_down.astype(int))\n",
    "\n",
    "print(\"\\n✓ All analysis complete – CSVs & PNGs in ./artifacts/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdab552",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
